\documentclass[%
	11pt,
	a4paper,
	utf8,
	%twocolumn
]{article}	

\usepackage{style_packages/podvoyskiy_article_extended}


\begin{document}
\title{Конспект по книге Гудфеллоу <<Глубокое обучение>>\footnote{Гудфеллоу Я., Бенджио И., Курвилль А. Глубокое обучение. -- М.: ДМК Пресс, 2018. -- 652 с. }}

\author{}

\date{}
\maketitle

\thispagestyle{fancy}

\tableofcontents

\section{Численные методы}

\section{Основы машинного обучения}

\subsection{Точечная оценка}

Точечное оценивание -- это попытка найти единственное <<наилучшее>> предсказание интересующей величины. Пусть $ \{ x^{(1)}, \ldots, x^{(m)} \} $ -- множество $ m $ независимых и одинаково распределенных точек. \emph{Точечной оценкой}, или \emph{статистикой}, называется любая функция этих данных
\begin{align*}
	\theta_m = g(x^{(1)}, \ldots, x^{(m)}).
\end{align*}

В этом определении не требуется, чтобы $ g $ возвращала значение, близкое к истинному значению $ \theta $, ни даже чтобы область значений $ g $ совпадала со множеством допустимых значений~$ \theta $.

Алгоритм $ k $-групповой перекрестной проверки применяется для оценивания ошибки обобщения алгоритма обучения $ A $, когда имеющийся набор данных $ \mathbb{D} $ \emph{слишком мал} для того, чтобы простое разделение на обучающий и тестовый или обучающий и контрольный наборы могло дать точную оченку ошибки обобщения, поскольку среднее значение потери $ L $ на малом тестовом наборе может иметь высокую дисперсию. 

\subsection{Смещение}

\emph{Смещение оценки} определяется следующим образом
\begin{align*}
	\text{bias}(\hat{\theta}_m) = \mathbb{E}( \hat{\theta}_m ) - \theta, 
\end{align*}
где математической ожидание вычисляется по данным (рассматривается как выборка из случайной величины), а $ \theta $ -- истинное значение параметра, которое определяет порождающее распределение.

Оценка $ \hat{\theta} $ называется \emph{несмещенной}, если 
\begin{align*}
	\text(\hat{\theta}_m) = 0, \ \text{т.е.} \ \mathbb{E}(\hat{\theta}_m) = \theta.
\end{align*}

Оценка $ \hat{\theta}_m $ называется \emph{асимптотически несмещенной}, если
\begin{align*}
	\lim_{m \to \infty} \text{bias}(\hat{\theta}_m) = 0, \ \text{т.е.} \lim_{m \to \infty} \mathbb{E}(\hat{\theta}_m) = \theta.
\end{align*}


\subsection{Дисперсия}

Для определения смещения мы вычисляли математичесвкое ожидание оценки, но точно так же можем вычислить и ее дисперсию. \emph{Дисперсией оценки} называется выражение
\begin{align*}
	\text{Var}(\hat{\theta}).
\end{align*}

\emph{Стандартной ошибкой} $ \text{SE}(\hat{\theta}) $ называется квадратный корень из дисперсии.

Воспользовавшись центральной предельной теоремой, согласно которой среднее имеет приблизительно нормальное распределение, можем применить стандартную ошибку для вычисления вероятности того, что истинное математическое ожидание находится в выбранном интервале. Например, \emph{95-процентный доверительный интервал} вокруг выборочного среднего (вокру оценки) $ \hat{\mu}_m = \dfrac{1}{m} \sum\limits_{k=1}^{n} x^{(i)} $ определяется формулой
\begin{align*}
	(\hat{\mu}_m - 1.96 \, \text{SE} (\hat{\mu}_m), \hat{\mu}_m + 1.96 \, \text{SE} (\hat{\mu}_m))
\end{align*}
при нормальном распределении со средним $ \hat{\mu}_m $ и дисперсией $ \text{SE}(\hat{\mu}_m)^2 $.

NB: В экспериментах по машинному обучению принято говорить, что алгоритм $ A $ лучше алгоритма $ B $, если верхняя граница 95-процентного доверительного интервала для ошибки алгоритма $ A $ меньше нижней границы 95-процентного доверительного интервала для ошибки алгоритма $ B $.

\subsection{Поиск компромисса между смещением и дисперсией для минимизации среднеквадратической ошибки}

Что, если имеются две оценки, у одной из которых больше смещение, а у другой дисперсия? Какую выбрать? 

Самый распространенный подход к выбору компромиссного решения -- воспользоваться \emph{перекрестной проверкой}. Эмпирически продемонстрировано, что перекрестная проверка дает отличные результаты во многих реальных задачах.

Можно также сравнить среднекваратическую ошибку (MSE) обеих оценок
\begin{align*}
	\text{MSE} = \mathbb{E} [ ( \hat{\theta}_m - \theta )^2] = \text{bias}(\hat{\theta}_m)^2 + \text{Var}(\hat{\theta}_m)
\end{align*}

Желательной является оценка с малой MSE, именно такие оценки держат под контролем и смещение, и дисперсию. Соотношение между смещением и дисперсией тесно связано с возникающими в машинном обучении понятиями емкости модели, недообучения и переобучения.

Если ошибка обобщения измеряется посредством MSE (и тогда смещение и дисперсия становятся важными компонентами ошибки обобщения), то увеличение емкости (то есть \emph{усложение модели}) влечет за собой \emph{повышение дисперсии} и \emph{снижение смещения}.


\subsection{Состоятельность}

Обычно нас интересует также поведение оценки по мере роста размера обучающего набора. В частности, мы хотим, чтобы при увеличении числа примеров точечные оценки сходились к истинным значениям соответствующих параметров.

Формально это записывается в виде (условие состоятельности)
\begin{align*}
	\hat{\theta}_m \stackrel{\mathbf{P}}{\longrightarrow} \theta, \ (m \to \infty)
\end{align*}

Иногда это условие называют \emph{слабой состоятельностью}, понимая под \emph{сильной состоятельностью} сходимость \emph{почти наверное} $ \hat{\theta} $ к $ \theta $.

{\color{blue}Состоятельность гарантирует, что смещение оценки уменьшается с ростом числа примеров}. Однако обратное неверно -- {\color{red}из асимптотической несмещенности не вытекает состоятельность}. Рассмотрим, к примеру, оценивание среднего $ \mu $ нормального распределения $ N(x; \mu, \sigma^2) $ по набору данных, содержащему $ m $ примеров: $ \{ x^{(1)}, \ldots, x^{(m)} \} $.

Можно было бы взять в качестве оценки первый пример: $ \hat{\theta} = x^{(i)} $. В таком случае $ \mathbb{E}(\hat{\theta})_m = \theta $, поэтому оценка является несмещенной вне зависимости от того, сколько примеров мы видели. Отсюда, конечно, следует, что оценка асимптотически несмещенная. Но она не является состояительной, т.к. \emph{неверно}, что $ \hat{\theta}_m \to \theta, \ (m \to \infty) $.

\subsection{Оценка максимального праводподобия}

Рассмотрим множества $ m $ примеров $ \mathbb{X} = \{ x^{(1)}, \ldots, x^{(m)} \} $, независимо выбираемых из неизвестного порождающего распределения $ p_{data}(x) $.

Обозначим $ p_{model}(x; \theta) $ параметрическое семейство распределений вероятности над одним и тем же пространством, индексированное параметром $ \theta $. 

Тогда оценка максимального правдоподобия для $ \theta $ определяется формулой
\begin{align*}
	\theta_{ML} = \argmax_{\theta} \, p_{model} (\mathbb{X}; \theta) = \argmax_{\theta} \, \prod_{i=1}^{m} p_{model}(x^{(i)}; \theta)
\end{align*}

Такое произведение большого числа вероятностей по ряду причин может быть неудобно. Например, оно подвержено \emph{потере значимости}. Для получения эквивалентной, но более удобной задачи оптимизации заметим, что взятие логарифма правдоподобия не изменяет $ \argmax $, но преобразует произведение в сумму
\begin{align*}
	\theta_{ML} = \argmax_{\theta} \, \sum_{i=1}^{m} \log p_{model}(x^{(i)}; \theta)
\end{align*}

Поскольку $ \argmax $ не изменяется при умножении функции стоимости на константу, мы можем разделить правую часть на $ m $ и получить выражение в виде математического ожидания относительно эмпирического распределения $ \hat{p}_{data} $, определяемого обучающими данными
\begin{align*}
	\theta_{ML} = \argmax_{\theta} \, \mathbb{E}_{x \, \sim \, \hat{p}_{data}} [\, \log p_{model} (x; \theta) \,]
\end{align*}

{\color{blue}Один из способов интерпретации оценки максимального правдоподобия состоит в том, чтобы рассматривать ее как минимизацию дивергенции (расхождения) Кульбака-Лейблера между этими эмпирическим распределением $ \hat{p}_{data} $, определяемым обучающим набором, и модельным распределением.}

Дивергенция Кульбака-Лейблера определяется формулой
\begin{align*}
	D_{KL}(\hat{p}_{data} \, || \, p_{model}) = \mathbb{E}_{ x \, \sim \, \hat{p}_{data} } [\, \log \hat{p}_{data}(x) - \log p_{model}(x) \, ]
\end{align*}

Первый член разности в квадратных скобках зависит только от порождающего данные процесса, но не от модели. Следовательно, при обучении модели, минимизирующей дивергенцию КЛ, мы должны минимизировать только величину
\begin{align*}
	- \mathbb{E}_{ x \, \sim \, \hat{p}_{data} }[\, \log p_{model}(x) \,],
\end{align*}
а это, конечно, то же самое, что максимизация величины $ \theta_{ML} = \argmax_{\theta} \, \mathbb{E}_{x \, \sim \, \hat{p}_{data}} [\, \log p_{model} (x; \theta) \,] $.

NB: То есть, другими словами задача максимизации правдоподобия эквивалентна задаче минимизации дивергенции Кульбака-Лейблера между эмпирическим распределением $ \hat{p}_{data} $ и модельным распределением $ p_{model} $.

\subsection{Метод опорных векторов}

Линейную функцию в методе опорных векторов можно переписать в виде
\begin{align*}
	w^{T} x + b = b + \sum_{i=1}^{m} \alpha_i x^{T} x^{(i)},
\end{align*}
где $ x^{(i)} $ -- обучающий пример, $ \alpha $ -- вектор коэффициентов.

Записав алгоритм обучения в таком виде, мы сможем заменить $ x $ результатом заданной функции признаков $ \varphi(x) $, а скалярное произведение -- функцией $ k(x, x^{(i)}) = \varphi(x) \cdot \varphi(x^{(i)}) $, которая называется ядром.

Заменив скалярное произведение вычислением ядра, мы можем делать предсказание, пользуясь функцией
\begin{align*}
	f(x) = b + \sum_{i} \alpha_i k(x, x^{(i)})
\end{align*}

Основанная на ядре функция в точности эквивалентна предварительной обработке путем применения $ \varphi(x) $ ко всем входным данным с последующим обучением линейной модели в новом преобразованном пространстве.

NB: Трюк с ядром полезен по двум причинам:
\begin{itemize}
	\item Во-первых, он позволяет обучать модели, \emph{нелинейно} зависящие от $ x $, применяя методы выпуклой оптимизации, о которых точно известно, что они сходятся эффективно
	
	\item  Во-вторых, \emph{ядерная функция} $ k $ часто допускает реализацию, значительно \emph{более эффективную с вычислительной точки зрения}, чем наивное построение двух векторов $ \varphi(x) $ и явное вычисление их скалярного произведения
\end{itemize}	

Главный недостаток ядерных методов -- тот факт, что сложность вычисления решающей функции линейно зависит от числа обучающих примеров, поскольку $ i $-ый пример вносит член $ \alpha_i k(x, x^{(i)}) $ в решающую функцию.

В методе опорных векторов эта проблема сглаживается тем, что обучаемый вектор $ \alpha $ содержит в основном нули. {\color{blue}Тогда для классификации нового примера \emph{требуется вычислить ядерную функцию только для обучающих примеров с ненулевыми} $ \alpha_i $.} Эти обучающие примеры и называются опорными векторами.

\subsection{Метод главных компонент}

Метод главных компонент находит ортогональное линейное преобразование, переводящее входные данные $ x $ в представление $ z $. 

Рассмотрим матрицу плана $ X $ размера $ m \times n $. Будем предполагать, что матемаческое ожидание данных $ \mathbb{E}[x] = 0 $. Если это не так, центрирования легко добиться, вычтя среднее из всех примеров на этапе предварительной обработки.

\emph{Несмещенная выборочная ковариационная матрица}, ассоциированная с $ X $, определяется по формуле
\begin{align*}
	Var[x] = \dfrac{1}{m - 1} X^T X
\end{align*}

PCA находит представление (посредством линейного преобразования) $ z = W^T x $, для которого $ Var[z] $ -- \emph{диагональная}.

Главные компоненты можно получить с помощью сингулярного разложения. Точнее, это правые сингулярные веркторы. Чтобы убедиться в этом, предположим, что $ W $ -- правые сингулярные векторы в разложении $ X = U \Sigma W^T $. Тогда исходное уравнение собственных векторов можно переписать в базисе $ W $
\begin{align*}
	X^T X = (U \Sigma W^T)^T U \Sigma W^T = W \Sigma^2 W^T
\end{align*}

Разложение SVD полезно для доказательства того, что PCA приводит к диагональной матрице $ Var[z] $. Применяя сингулярное разложение $ X $, мы можем выразить дисперсию $ X $ в виде
\begin{align*}
	Var[x] = \dfrac{1}{m - 1} X^T X = \dfrac{1}{m - 1} (U \Sigma^2 W^T)^T U \Sigma W^T = \dfrac{1}{m - 1} W \Sigma^2 W^T,
\end{align*}
где используется тот факт, что $ U^T U = I $, поскольку матрца $ U $ в сингулярном разложении по определению ортогональная. Отсюда следует, что ковариационная матрица $ z $ диагональная
\begin{align*}
	Var[z] = \dfrac{1}{m - 1} Z^T Z = \dfrac{1}{m - 1}W^T X^T X W = \dfrac{1}{m-1} W^T W \Sigma^2 W^T W = \dfrac{1}{m - 1}\Sigma^2
\end{align*}

На этот раз мы воспользовались тем, что $ W^T W = I $ -- опять же по определению сингулярного разложения.

Проведенный анализ показывает, что {\color{blue}представление, полученное в результате проецирования данных $ x $ на $ z $ посредством линейного преобразования $ W $, имеет \emph{диагональную ковариационную матрицу} $ \Sigma^2 $. А отсюда сразу вытекает, что \emph{взаимная корреляция отдельных элементов} $ z $ \emph{равна нулю}.}

\subsection{Стохастический градиентный спуск}

Стохастический градиентный спуск (СГС) имеет важные применения и за пределами глубокого обучения. Это основной способ обучения \emph{больших линейных моделей} на очень больших наборах данных. Для модели фиксированноо размера стоимость одного шага СГС не зависит от размера обучающего набора $ m $. Количество шагов до достижения сходимости обычно возрастает с ростом размера обучающего набора. Но когда $ m $ стремится к бесконечности, модель в итоге сходится к наилучшей возможной ошибке тестирования еще до того, как СГС проверил каждый пример из обучающего набора. Дальнейшее увеличение $ m $ не увеличивает время обуччения, необходимое для достижения наилучшей ошибки тестирования. {\color{blue}С этой точки зрения можно сказать, что асимптотическая стоимость обучения модели методом СГС как функции от $ m $ имеет порядок~$ O(1) $.}

\subsection{Условное логарифмическое правдоподобие и среднеквадратическая ошибка}

\emph{Линейую регрессию} можно интерпретировать как \emph{нахождение оценки максимального правдоподобия}. Будем считать, что цель не в том, чтобы вернуть одно предсказание $ \hat{y} $, а чтобы построить модель, порождающую условное распределение $ p(y | \mathbf{x}) $. Цель алгоритма обучения теперь -- аппроксимировать $ p(y | \mathbf{x}) $, подогнав его под все эти разные значения $ y $, совместимые с $ \mathbf{x} $.

Для вывода такого же алгоритма линейной регрессии, как и раньше, определим
$$
p(y | \mathbf{x}) = N(y; \hat{y}(\mathbf{x}, \mathbf{w}), \sigma^2)
$$

Функкция $ \hat{y}(\mathbf{x}; \mathbf{w}) $ дает предсказание среднего значения нормального распределения. В этом примере мы предполагаем, что дисперсия фиксирована и равна константе $ \sigma^2 $. Поскольку предполагается, что примеры независимы и одинаково распределены, то условное логарифмическое правдоподобие записывается в виде
\begin{align*}
	\sum_{i=1}^{m} \log p(y^{(i)} | \mathbf{x}^{(i)}; {\theta}) = -m \, \log \sigma - \dfrac{m}{2} \log 2 \pi - \sum_{i=1}^{m} \dfrac{ \| \hat{y}^{(i)} - y^{(i)} \|^2 }{ 2 \sigma^2 },
\end{align*}
где $ \hat{y}^{(i)} $ -- результат линейной регрессии для $ i $-ого примера $ \mathbf{x}^{(i)} $, а $ m $ -- число обучающих примеров.

Сравнивая логарифмическое правдоподобие со среднеквадратической ошибкой
\begin{align*}
	\text{MSE}_{train} = \dfrac{1}{m} \sum_{i=1}^{m} \| \hat{y}^{(i)} - y^{(i)} \|^2,
\end{align*}
мы сразу же видим, что \emph{максимизация логарифмического правдоподобия} относительно $ \mathbf{w} $ дает ту же оценку параметров $ \mathbf{w} $, что \emph{минимизация среднеквадратической ошибки}.

Значения этих критериев различны, но положение оптимума совпадает. Это служит обоснованием использования среднеквадратической ошибки в качестве оценки максимального правдоподобия.

\section{Глубокие сети прямого распространения}

\subsection{Обучение условных распределений с помощью максимального правдоподобия}

Большинство современных нейронных сетей обучается \emph{с помощью максимального правдоподобия}. Это означает, что {\color{blue}в качестве функции стоимости берется отрицательное логарифмическое правдоподобие}, которое можно эквивалентно описать как перекрестную энтропию между обучающими данными и распределением модели
\begin{align*}
	J(\theta) = - \mathbb{E}_{x, y \sim \hat{p}_{\text{data}}} \log p_{\text{model}} (\mathbf{y} | \mathbf{x})
\end{align*}

Одно необычное свойство перекрестной энтропии, используемой при вычислении оценки максимального правдоподобия, заключается в том, что для типичных встречающихся на практике моделей у нее, как правило, нет минимального значения. Если выходная величина дискретна, то в большинстве моделей параметризация устроена так, что модель неспособна представить вероятность 0 или 1, но может подойти к ней сколь угодно близко. Примером может служить логистическая регрессия.

\subsection{Сигмоидные блоки и выходное распределение Бернулли}

Во многих задачах требуется предсказывать значение бинарной величины $ y $. В таком виде можно представить задачу классификации с двумя классами.

Подход на основе максимального правдоподобия заключается в определении распределения Бернулли величины $ y $ при условии $ \mathbf{x} $.

{\color{blue}Градиент $ \mathbf{0} $ обычно приводит к проблемам, потому что у алгоритма обучения нет никаких указаний на то, как улучшить параметры.}

Лучше применять другой подход, который гарантирует, что градиент обязательно будет достаточно большим, если модель дает неверный ответ. Этот подход основан на использовании сигмоидных выходных блоков в сочетании с максимальным правдоподобием.

Сигмоидный выходной блок
\begin{align*}
	\hat{y} = \sigma (\mathbf{w}^T \mathbf{h} + b),
\end{align*}
где $ \sigma $ -- логистическая сигмоида.

\subsection{Блоки softmax и категориальное выходное распределение}

Если требуется представить распределение вероятности дискретной случайной величины, принимающей $ n $ значений, то можно воспользоваться функцией softmax. Ее можно рассматривать как обобщение сигмоиды, которая использовалась для представления распределения бинарной величины.

Функция softmax чаще всего используется как выход классификатора для представления распределения вероятности $ n $ классов. Реже функция softmax используется внутри самой модели.

Как и сигмоида, функция активации softmax склонна к насыщению. У сигмоиды всего один выход, и она насыщается, когда абсолютная величина аргумента очень велика. У softmax выходных значений несколько. Они насыщаются, когда велика абсолютная величина разностей между входными значениями.

softmax -- это сглаженный вариант $ \argmax $. Сложность спектрального разложения матрицы $ d \times d $ имеет порядок $ O(d^3) $.

\subsection{Скрытые блоки}

Некоторые скрытые блоки, не являются всюду дифференцируемыми. Например, функция линейной ректификации (ReLU) $ g(z) = \max \{ 0, z \} $ не дифференцируема в точке $ z = 0 $. Может показаться, что из-за этого $ g $ непригодна для работы с алгоритмами обучения градиентными методами. Но на практике градиентный спуск работает для таких моделей машинного обучения достаточно хорошо. Отчасти это связано с тем, что {\color{blue}алгоритмы обучения нейронных сетей обычно не достигают локального минимума функции стоимости}, а просто находят достаточно малое значение. {\color{blue}Поскольку мы не ожидаем, что обучение выйдет на точку, где градиент равен 0, то можно смириться с тем, что минимум функции стоимости соответствует точкам, в которых градиент не определен.} Недифференцируемые скрытые блоки обычно не дифференцируемы лишь в немногих точках. В общем случае функцию $ g(z) $ имеет производную слева, определяемую коэффициентом наклона функции слева от $ z $, и аналогично производную справа. Функция дифференцируема в точке $ z $, только если производные слева и справа определены и равным между собой. Для функции $ g(z) = \max \{ 0, z \} $ производная слева в точке $ z = 0 $ равна 0, а производная справа равна 1. В программных реализациях обучения нейронной сети обычно возвращается какая-то односторонняя производная, а не сообщается, что производная не определена и не возбуждается исключение. Эврестически это можно оправдать, заметив, что градиентная оптимизация на цифровом компьютере в любом случае подвержена численным погрешностям. Когда мы просим вычислить $ g(0) $, крайне маловероятно, что истинное значение действительно равно 0. Скорее всего, это какое-то малое значение, округленное до 0.

{\color{blue}Важно, что на практике можно \emph{спокойно игнорировать недифференцируемость функции активации} скрытых блоков.}

\subsubsection{Блоки линейной ректификации и их обобщения}

В блоке линейной ректификации используется функция активации $ g(z) = \max \{ 0, z \} $. Эти блоки легко оптимизировать, потому что они очень похожи на линейные. Разница только в том, что блок линейной ректификации в половине своей области определения выводит 0. {\color{blue}Поэтому производная блока линейной ректификации остается большой всюду, \emph{где блок активен}}.

Блоки линейной ректификации обычно применяются \emph{после} аффинного преобразования
\begin{align*}
	h = g(W^T x + b)
\end{align*}

При инициализации параметров аффинного преобразования рекомендуется присваивать всем элементам $ b $ небольшое положительное значение, например, 0.1. Тогда блок линейной ректификации в начальный момент с большой вероятностью окажется активен для большинства обучающих примеров, и производная будет отлична от нуля.

{\color{red}Недостатком блоков линейной ректификации является невозможность обучить их градиентными методами на примерах, для которых функция активации блока равна нулю}. Различные обобщения гарантируют, что градиент имеется в любой точке.

Три обобщения блоков линейной ректификации основаны на использовании ненулевого углового коэффициента $ \alpha_i $, когда $ z_i < 0: h_i = \max(0, z_i) + \alpha_i \min(0, z_i) $.

В случае абсолютной ректификации берутся фиксированные значения $ \alpha_i = -1 $, так что $ g(z) = | z | $. Такая функция активации используется при распозновании объектов в изображении, где имеет смысл искать признаки, инвариантные относительно изменения полярности освещения. Другие обобщения находят более широкие применения. В случае ReLU с утечкой $ \alpha_i $ принимаются равными фиксированному малому значению, например, 0.01, а в случае параметрического ReLU $ \alpha_i $ считается обучаемым параметром.

Блоки линейной ректификации и все их обобщения основаны на принципе, согласно которому модель проще обучить, если ее поведение близко к линейному.

\subsubsection{Логистическая сигмоида и гиперболический тангенс}

Блоки линейной ректификации стали использоваться сравнительно недавно, а раньше большинство нейронных сетей в роли функции активации применялась логистическая сигмоида или гиперболический тангенс.

Сигмоидные блоки в качестве выходных предсказывают вероятность того, что бинарная величина равна 1. В отличие от кусочно-линейных, сигмоидные блоки близки к асимптоте в большей части своей области определения -- приближаются к высокому значению, когда $ z $ стремится к бесконечности, и к низкому, когда $ z $ стремится к минус бесконечности. Высокой чувствительностью они обладают только в окрестности нуля. Из-за \emph{насыщения} сигмоидальных блоков \emph{градиентное обучение сильно затруднено}. Поэтому использование их в качестве скрытых блоков в сетях прямого распространения ныне не рекомендуется. 

Если использовать сигмоидальную функцию активации необходимо, то лучше взять не логистическую сигмоиду, а гиперболический тангенс. Он ближе к тождественной функции в том смысле, что $ \tanh(0) = 0 $, тогда как $ \sigma(0) = 1/2 $.

Поскольку $ \tanh $ походит на тождественную функцию в окрестности нуля, обучение глубокой нейронной сети $ \hat{y} = w^T \tanh (U^T \tanh (V^T x)) $ напоминает обучение линейной модели $ \hat{y} = w^T U^T V^T x $, при условии, что сигналы активации сети удается удерживать на низком уровне. При этом обучение сети с функцией активации $ \tanh $ упрощается.

Сигмоидальные функции активации все же применяются, но не в сетях прямого распространения. К реккурнетным сетям, многим вероятностным моделям и некоторым автокодировщикам предъявляются дополнительные требования, исключающие использование кусочно-линейных функций.

\subsection{Правило дифференцирования сложной функции}

Это правило применяется для вычисления производных функций, являющихся композициями других функций, чьи производные известны. 

Пусть $ \mathbf{x} \in \mathbb{R}^m, \mathbf{y} \in \mathbb{R}^n $, $ g $ отображает $ \mathbb{R}^m $ в $ \mathbb{R}^n $, а $ f $ отображает $ \mathbb{R}^n $ в $ \mathbb{R} $. Тогда правило дифференцирования сложной функции в векторной форме запишется в виде
\begin{align*}
	\nabla_x z = \Big( \dfrac{ \partial \mathbf{y} }{ \partial \mathbf{x} } \Big)^T \nabla_y z,
\end{align*}
где $ \partial \mathbf{y} / \partial \mathbf{x} $ -- \emph{матрица Якоби} функции $ g $ размера $ n \times m $.

Отсюда видно, что градиент по переменной $ \mathbf{x} $ можно получить, умножив матрицу Якоби $ \partial \mathbf{y} / \partial \mathbf{x} $ на градиент $ \nabla_y z $.

Обычно алгоритм обратного распространения применяется к тензорам произвольной размерности, а не просто к векторам. Концептуально это то же самое, что применение к векторам. Разница только в том, как числа организуются в сетке для представления тензора. Можно вообразить, что тензор сериализуается в вектор перед обратным распространением, затем вычисляется векторозначный градиент, после чего градиент снова преобразуется в тензор.

В таком переупорядоченном представлении обратное распространение -- это все то же умножение якобиана на градиенты.

Для обозначения градиента значения $ z $ относительно тензора $ \mathbf{X} $ мы пишем $ \nabla_\mathbf{X} z $, как если бы $ \mathbf{X} $ был просто вектором. Теперь индексы элементов $ \mathbf{X} $ составные -- например, трехмерный тензор индексируется тремя координатами. Мы можем абстрагироваться от этого различия, считая, что одна переменная $ i $ представляет целый кортеж индексов. Для любого возможного индексного кортежа
$ i $ $ (\nabla_\mathbf{X} z)_i $ обозначает частную производную $ \partial z / \partial \mathbf{X}_i $ -- точно так же, как для любого целого индекса $ i $ $ (\nabla_x z)_i$ обозначает $ \partial z / \partial x_i $. В этих обозначениях можно записать правило дифференцирования сложной функции в применении к тензорам. Если $ \mathbf{Y} = g(\mathbf{X}) $ и $ z = f(\mathbf{Y}) $, то
\begin{align*}
	\nabla_\mathbf{X} z = \sum_j (\nabla_\mathbf{X} Y_j) \, \dfrac{\partial z}{ \partial Y_j }
\end{align*}

Алгоритм обратного распространения был разработан, чтобы избежать многократного вычисления одного и того же выражения при дифференцировании сложной функции. Из-за таких повторов время выполнения наивного алгоритма могло расти экспоненциально. Теперь, можно оценить вычислительную сложность алгоритма обратного распространения.

Если предположить, что стоимость вычисления всех операций приблизительно одинакова, то вычислительную сложность можно проанализировать в терминах количества выполненных операций. Следует помнить, что под единицей мы понимаем базовую единицу графа вычислений, в действительности она может состоять из нескольких арифметических операций (например, умножение матриц может считаться одной операцией в графе). Вычисление градиента в графе с $ n $ вершинами никогда не приводит к выполнению или сохранению результатов более $ O(n^2) $. Здесь мы подсчитываем в графе вычислений, а не отдельные аппаратные операции, поэтому важно понимать, что время выполнения разных операций может значительно различаться. 

Легко видеть, что для вычисления градиента требуется не более $ O(n^2) $ операций, потому что на этапе прямого распространения в худшем случае будут обсчитаны все $ n $ вершин исходного графа (в зависимости от того, какие значения мы хотим вычислить, может потребоваться обойти весь граф). 

{\color{blue}Поскольку граф вычислений -- это ориентированный ациклический граф, число ребер в нем не более $ O(n^2) $.}  Для типичных графов, встречающихся на практике, ситуация даже лучше. В большинстве нейронных сетей функции стоимости имеют в основном цепную структуру, так что сложность обратного распространения равна $ O(n) $. Это намного лучше, чем наивный подход, при котором число обрабатываемых вершин иногда растет \emph{экспонециально}!

Откуда возникает экспоненциальный рост, можно понять, раскрыв и переписав правило дифференцирования сложной функции без рекурсии (здесь сумма по путям $ u^{(\pi_1, u^{(\pi_2)}), \ldots, u^{(\pi_t)}} $ из $ \pi_1 = j $ в $ \pi_t = n $)
\begin{align*}
	\dfrac{ \partial u^{(n)} }{ \partial u^{(j)} } = \sum \prod_{k=2}^{t}  \dfrac{ \partial u^{(\pi_k)} }{ \partial u^{(\pi_{k - 1})} }
\end{align*}

Поскольку количество путей из вершины $ j $ в вершину $ n $ может экспоненциально зависит от длины пути, то число слагаемых в этой сумме, равное числу таких путей, может расти экспоненциально с увеличением глубины графа прямого распространения. Такая высокая сложность связана с многократным вычислением $ \partial u^{(i)} / \partial u^{(j)} $. Чтобы \emph{избежать повторных вычислений}, мы можем рассматривать \emph{обратное распространение как алгоритм заполнения таблицы}, в которой храняться промежуточные результаты $ \partial u^{(n)} / \partial u^{(i)} $.

Каждой вершине графа соответствует элемент таблицы, в котором хранится градиент для этой вершины. Заполняя таблицу в определенном порядке, алгоритм обратного распространения избегает повторного вычисления многих ошибок подвыражений. Такую стратегию заполнения таблицы иногда называют динамическим программированием.

\subsection{Регуляризация в глубоком обучении}

\subsubsection{Штрафы по норме параметров}

Многие подходы к регляризации основаны на ограничении емкости моделей путем прибавления  штрафа по норме параметра $ \Omega(\theta) $ к целевой функции $ J $
\begin{align*}
	\tilde{J}(\theta; X, y) = J(\theta; X, y) + \alpha \Omega(\theta),
\end{align*}
где $ \alpha \in [0, \infty] $ -- гиперпараметр, задающий вес члена $ \Omega $, штрафующего по норме, относительно стандратной целевой функции $ J $. Чем больше значение $ \alpha $, тем сильнее регуляризация.

В нейронных сетях мы обычно предпочитаем штрафовать по норме \emph{только веса} аффинного преобразования в каждом слое, оставляя смещения нерегуляризованными.

Регуляризация параметров смещения может стать причиной значительного недообучения. 

\paragraph{Регуляризация парамтеров по норме $ L_2 $}

Полная целевая функция с регуляризацияей Тихонова имеет вид
\begin{align*}
	\tilde{J}(w; X, y) = \dfrac{\alpha}{2} w^T w + J(w; X, y),
\end{align*}
а градиент по параметрам 
\begin{align*}
	\nabla_w \tilde{J}(w; X, y) = \alpha w + \nabla_w J(w; X, y)
\end{align*}

Один шаг обновления весов с целью уменьшения градиента имеет вид
\begin{align*}
	w \leftarrow w - \varepsilon \, (\alpha w + \nabla_w J(w; X, y))
\end{align*}

То же самое можно переписать в виде
\begin{align*}
	w \leftarrow (1 - \varepsilon \alpha) w - \varepsilon \nabla_w J(w; X, y)
\end{align*}

Как видим, добавление члена сложения весов изменило правило обучения: теперь мы на каждом шаге умножаем вектор весов на постоянный коэффициент, меньший 1, перед тем как выполнить стандартное обновление градиента. 

Еще упростим анализ, предположив квадратичную аппроксимацию целевой функции в окрестности того значения весов, при котором достигается минимальная стоимость обучения без регуляризации. Если целевая функция действительно квадратичная, как в случае модели линейной регрессии со среднеквадратической ошибкой, то такая аппроксимация идеальна. Аппроксимация $ \tilde{J} $ описывается формулой
\begin{align*}
	\hat{J}(\theta) = J(w^*) + 1/2 (w - w^*)^T H(w - w^*),
\end{align*}
где $ H $ -- матрица Гессе $ J $ относительно $ w $, вычисленная в точке $ w^* $. В этой квадратичной аппроксимации нет члена первого порядка, потому что $ w^* $, по определению, точка минимума, в которой градиент обращается в нуль. 

Минимум $ \hat{J} $ достигается там, где градиент
\begin{align*}
	\nabla_w \hat{J}(w) = H(w - w^*) = 0
\end{align*}

Чтобы изучить эффект снижения весов, прибавим градиент снижения весов. Теперь мы можем найти из него минимум регуляризованного варианта $ \hat{J} $. Обозначим $ \tilde{w} $ положение точки минимума.

\begin{align*}
	\alpha \tilde{w} + H (\tilde{w} - w^*) = 0 \\
	(H + \alpha I) \tilde{w} = H w^* \\
	\tilde{w} = (H + \alpha I)^{-1} H w^*
\end{align*}

Поскольку матрица $ H $ вешественная и симметричная, мы можем разложить ее в произведение диагональной матрицы $ \Lambda $ и ортогональной матрицы собственных векторов $ Q $
\begin{align*}
	H = Q \Lambda Q^T
\end{align*}

Тогда
\begin{align*}
	\tilde{w} = (Q \Lambda Q^T + \alpha I)^{-1} Q \Lambda Q^T w^* = Q(\Lambda + \alpha I)^{-1} \Lambda Q^T w^*.
\end{align*}

Компонента $ w^* $, параллельная $ i $-ому собственному вектору $ H $, умножается на коэффициент $ \lambda_i / (\lambda_i + \alpha) $. Вдоль направлений, для которых собственные значения $ H $ относительно велики, например, когда $ \lambda_i \gg \alpha $, эффект регуляризации сравнительно мал. Те же компоненты, для которых $ \lambda_i \ll \alpha $, сжимаются почти до нуля.

Если направление не дает вклада в уменьшение целевой функции, то собственное значение гессиана мало, т.е. движение в этом направялении не приводит к заметному возрастнию градиента. Компоненты вектора весов, соответствующие таким малозначным направлениям, снижаются почти до нуля благодаря использованию регуляризации в ходе обучения.

Рассмотрим линейную регрессию, в которой истинная функция стоимости квадратическая. Для линейной регрессии функция стоимости равна сумме квадратов ошибок
\begin{align*}
	(x w - y)^T (X w - y)
\end{align*}

После добавления $ L_2 $-регуляризации целевая функция принимает вид
\begin{align*}
	(X w - y)^T (X w -y) + \dfrac{1}{2} w^T w
\end{align*}

В результате \emph{нормальные уравнения}, из которых ищется решение
\begin{align*}
	w = (X^T X)^{-1} X^T y
\end{align*}
принимают вид
\begin{align*}
	w = (X^T X + \alpha I)^{-1} X^T y
\end{align*}

Матрица $ X^T X $ пропорциональна \emph{ковариационной матрице} $ 1 / m X^T X $. Применение $ L_2 $-регуля-ризации заменяет эту матрицу на $ (X^T X + \alpha I)^{-1} $. Новая матрица отличается от исходной только прибавлением $ \alpha $ ко всем диагональным элементам. \emph{Диагональные элементы} этой матрицы соответствуют \emph{дисперсии каждого входного признака}. 

Таким образом, $ L_2 $-\emph{регуляризация} заставляет алгоритмы обучения <<воспринимать>> вход $ X $ \emph{как имеющий более высокую дисперсию} и, следовательно, уменьшать веса тех признаков, для которых ковариация с выходными метками мала, по сравнению с добавленной дисперсией.

NB: $ L_2 $-регуляризация занижает веса признаков, которые обнаруживают слабую ковариацию с целевой меткой.

\paragraph{$ L_1 $-регуляризация}

Формально $ L_1 $-регуляризация параметров модели $ w $ определяется по формуле
\begin{align*}
	\Omega(\theta) = \| w \|_1 = \sum_i | w_i |,
\end{align*}
т.е. как сумма абсолютных величин отдельных параметров.

Регуляризованная целевая функция описывается формулой
\begin{align*}
	\tilde{J}(w; X, y) = \alpha \| w \|_1 + J(w; X, y),
\end{align*}
а ее градиент (точнее, \emph{частичный градиент}) равен
\begin{align*}
	\nabla_w \tilde{J}(w; X, y) = \alpha \, \text{sign}(w) + \nabla_w J(w; X, y),
\end{align*}
где $ \text{sign}(w) $ означает, что функция sign применяется к каждому элементу $ w $.

Теперь вклад регуляризации в градиент уже не масштабируется линейно с ростом каждого $ w_i $, а описывается постоянным слагаемым, знак которого совпадает с $ \text{sign}(w_i) $. Один из следствий является тот факт, что мы уже не получим изящных алгебраических выражений квадратичной аппроксимации $ J(X; y, w) $, как в случае $ L_2 $-регуляризации.

Квадратичную аппроксимацию $ L_1 $-регуляризованной целевой функции можно представить в виде суммы по параметрам
\begin{align*}
	\hat{J}(w; X, y) = J(w; X, y) + \sum_i \Big[ \, \dfrac{1}{2} H_{i, i} (w_i - w_i^*)^2 + \alpha |w_i| \, \Big]
\end{align*}

У задачи минимизации этой приближенной функции стоимости имеется аналитическое решение (для каждого измерения $ i $) вида
\begin{align*}
	w_i = \text{sign}(w_i^*) \max \Big[ |w_i^*| - \dfrac{\alpha}{H_{i, i}}, 0 \Big]
\end{align*}

Предполагается, что матрица Гессе диагональная $ H = \text{diag}([H_{1, 1}, \ldots, H_{n, n}]) $, где все $ H_{i, i} > 0 $.

Предположим, что $ w_i^* > 0 $ для всех $ i $. Тогда есть два случая:
\begin{enumerate}
	\item $ w_i^* \leqslant \alpha / H_{i, i} $. Тогда оптимальное значение $ w_i $ для регуляризованной целевой функции будет просто $ w_i = 0 $,
	
	\item $ w_i^* > \dfrac{\alpha}{H_{i, i}} $. Тогда регуляризация не сдвигает оптимальное значение $ w_i $ в нуль, а просто смещает его в этом направлении на расстояние $ \alpha / H_{i, i} $. 
\end{enumerate}

Аналогичное рассуждение проходит, когда $ w_i^* < 0 $, только $ L_1 $-штраф увеличивает $ w_i $ на $ \alpha / H_{i, i} $ или обращает в 0.

По сравнению с $ L_2 $-регуляризацией, $ L_1 $-регуляризация дает более \emph{разреженное} решение. В этом контексте под разреженностью понимается тот факт, что у некоторых параметров оптимальное значение равно 0.

Свойство разреженности, присущее $ L_1 $-регуляризации, активно эксплуатировалось как механизм отбора признаков, идея которого состоит в том, чтобы упростить задачу машинного обучения за счет выбора некоторого подмножества располагаемых признаков. В частности, хорошо известная модель LASSO (Least Absolute Shrinkage and Selection Operator) объединяет $ L_1 $-штраф с линейной моделью и среднеквадратической функцией стоимости. Благодаря $ L_1 $-штрафу некоторые веса обращаются в 0, и соответствующие им признаки отбрасываются.

Многие \emph{стратегии регуляризации} можно интерпретировать как \emph{байесовский вывод} на основе \emph{оценки апостериорного максимума} (MAP). В частности, $ L_2 $-регуляризация эквивалента байесовскому выводу на основе MAP с \emph{априорным нормальным распределением весов}. В случае $ L_1 $-регуляризации штраф $ \alpha \Omega(w) = \alpha \sum_i | w_i | $, применяемый для регуляризации функции стоимости, эквивалентен члену, содержащему логарифм априорного расределения, который максимизируется байесовским выводом на основе MAP, когда в качестве \emph{априорного} используется \emph{изотропное распределение Лапласа векторов} $ w \in \mathbb{R}^n $.

\subsection{Регуляризация и неопределенные задачи}

В некоторых случаях без регуляризации просто невозможно корректно поставить задачу машинного обучения. Многие линейные модели в машинном обучении, в т.ч. линейная регрессия, включают обращение матрицы $ X^TX $. \emph{Это невозможно, если} $ X^T X $ \emph{сингулярная}.

Матрица может оказаться сингулярной, если порождающее распределение действительно не имеет дисперсии в некотором направлении, или если в некотором направении не наблюдается дисперсии, потому что число примеров (строк $ X $) меньше числа входных признаков (столбцов $ X $). В таком случае во многих вариантах регуляризации прибегают к обращению матрицы $ X^T X + \alpha I $. Гарантируется, что такая регуляризованная матрица обратима.

NB: {\color{blue}регуляризованная матрица $ (X^T X + \alpha I) $ обратима всегда!}

\subsection{Ранняя остановка}

Каким образом ранняя остановка выступает в роли регуляризатора. Мы уже не раз отмечали, что ранняя остановка ялвяется стратегией регуляризации, но в обоснование этого заявления привели только кривые обучения, на которых ошибка на контрольном наборе имеет U-образную форму. А каков истинный механизм регуляризации модели с помощью ранней остановки?

Допустим, что выбрано $ \tau $ шагов оптимизации (что соответствует $ \tau $ итерациям обучения) и скорость обучения $ \varepsilon $. Произведение $ \varepsilon \cdot \tau $ можно рассматривать как \emph{меру эффективной емкости}. В предположении, что градиент ограничен, наложение ограничений на число итераций и скорость обучения лимитируют область пространства параметров, достижимую из $ \theta_0 $. В этом смысле $ \varepsilon \cdot \tau $ ведет себя как величина, обратная коэффициенту снижения весов.

То есть чем больше итераций отводится для обучения или чем выше скорость обучения, тем сложнее модель и потому тем слабее регуляризация.

Значения параметров, соответствующие направялениям \emph{сильной кривизны} целевой функции, \emph{регуляризируются меньше}, чем в направлениях меньшей кривизны.

Конечно, ранняя остановка -- больше, чем простое ограничение на длину траектории; \emph{ранняя остановка} обычно подразумевает наблюдение за ошибкой на \emph{контрольном наборе}, чтобы оборвать траекторию в удачной точке пространства.

Поэтому, по сравнению со снижением весов, у ранней остановки есть то преимущество, что она автоматически определяет правильную степень регуляризации, тогда как при использовании снижения весов требуется много экспериментов с разными значениями-гиперпараметрами.

\subsection{Баггинг и другие ансамблевые методы}

Баггинг -- метод умеьшения ошибки обобщения путем комбинирования нескольких моделей. Идея заключается в том, чтобы раздельно обучить разные модели, а затем организовать их голосования за результаты на тестовых примерах. Это частный случай общей стратегии машинного обучения -- усреднения моделей. Методы, в которых эта стратегия используется, называется ансамблевыми методами.

В качестве примера рассмотрим набор из $ k $ моделей регрессии. Предположим, что каждая модель делает ошибку $ \varepsilon_i $ на каждом примере, причем ошибки имеют многомерное нормальное распределение с нулевым средним, дисперсиями $ \mathbb{E}[\varepsilon_i^2] = v $ и ковариациями $ \mathbb{E}[v_i v_j] = c $. Тогда ошибка, полученная в результате усреднения предсказаний всего ансамбля моделей, равна $ (1 / k) \sum_i \varepsilon_i $.

Математическое ожидание квадрата ошибки ансамблевого предиктора равно
\begin{align*}
	\mathbb{E} \Big[ \big( \dfrac{1}{k} \sum_i \varepsilon_i \big)^2 \Big] = \dfrac{1}{k^2} \mathbb{E} \Big[ \sum_i \big( \varepsilon_i^2 + \sum_{j \neq i} \varepsilon_i \varepsilon_j \big) \Big] = \dfrac{1}{k} v + \dfrac{k - 1}{k} c.
\end{align*}

В случае, когда ошибки идеально коррелированы, т.е. $ c = v $, среднеквадратическая ошибка сводится к $ v $, так что усреднение моделей ничем не помогает. В случае, когда ошибки вообще некоррелированы, т.е. $ c = 0 $, среднеквадратическая ошибка ансамбля равна всего $ (1 / k) v $ и, значет, линейно убывает с ростом размера ансамбля. Иными словами, в среднем показывает качество не хуже любого из его членов, а если члены совершают независимые ошибки, то ансамбль работает значительно лучше своих членов.

Точнее, баггинг подразумевает построение $ k $ разных наборов данных. В каждом наборе столько же примеров, сколько в исходном, но строятся они путем \emph{выборки с возвращением} из исходного набора. Это означает, что с высокой вероятностью в каждом наборе данных отуствуют некоторые примеры из исходного набора и присутствуют несколько дубликатов (в среднем, если размер результирующего набора равен размеру исходного, в него попадает две трети примеров из исходного набора). 

Затем $ i $-ая модель обучается на $ i $-ом наборе данных. Различия в составе примеров, включеных в набор, обуславливают различия между обученными моделями.

{\color{blue}Нейронные сети дают настолько широкое разнообразие решений, что усреднение моделей может оказаться выгодным, даже если все модели обучались на одном и том же наборе данных.} Различия, обусловленные случайной инициализацией, случайным выбором мини-пакетов, разными гиперпараметрами и результатами недетрминированной реализации нейронной сети, зачастую достаточны, чтобы различные члены ансамбля допускали частично независимые ошибки.

Усреднение моделей -- исключительно мощный и надежный метод \emph{уменьшения ошибки обобщения}.

\subsection{Прореживание}

Прореживание (dropout) -- это вычислительно недорогой, но мощный метод \emph{регуляризации} широкого семейства моделей. Прореживание предлагает дешевую аппроксимацию обучения и вычисления баггингового ансамбля экспоненциально большого числа нейронных сетей.

Точнее говоря, в процессе прореживания обучается \emph{ансамбль}, состоящий из \emph{подсетей}, получаемых \emph{удалением невыходных нейронов сетей}.

Напомним, что для обучения методом баггинга мы определяем $ k $ различных моделей, строим $ k $ различных наборов данных путем выборки с возвращением из обучающего набора, а затем обучаем $ i $-ю модель на $ i $-ом наборе.

Цель \emph{прореживания} -- аппроксимировать этот процесс на экспоненциально большом числе нейронных сетей. Точнее говоря, для обучения методом прореживания мы используем алгоритм, основанный на мини-пакетах, который делает небольшие шаги, например алгоритм стохастического градиентного спуска. При загрузке \emph{каждого примера} в мини-пакет мы случайным образом генерируем битовую маску, применяемую ко всем \emph{входным} и \emph{скрытым} блокам сети. Элемент маски для каждого блока выбирается независимо от всех остальных.

Вероятность включить в маску значение 1 (означающее, что соответствующий блок включается) -- гиперпараметр, фиксируемый до начала обучения, а не функция текущего значения параметров модели или входного примера. Обычно входной блок включается с вероятностью 0.8, а скрытый -- с вероятностью 0.5. Затем, как обычно, производится прямое распространение, обратное распространение и обновление.

Поскольку обычно принимается вероятность включения 0.5, то правило масштабирования весов сводится к делению весов пополам в конце обучения, после чего модель используется как обычно. Как бы то ни было, цель состоит в том, чтобы ожидаемому суммарному входу в блок на этапе тестирования был приблизительно равен ожидаемому суммарному входу в тот же блок на этапе обучения, несмотря на то что в среднем половина блоков во время обучения отсутствует.

Прореживание эффективнее других стандартных вычислительно недорогих регуляризаторов: снижения весов, фильтрации с ограничениями по норме и разреженной активации. Дальнейшего улучшения можно добиться, \emph{комбинируя} прореживание с другими видами регуляризации.

У прореживания есть еще одно важное преимущество: оно не налагает существенных ограничений на тип модели или процедуру обучения. Оно одинаково хорошо рабоатет практически с любой моделью, если в ней используется распределенное представление и ее можно обучить методом стохастического градиентного спуска. 

{\color{red} Хотя стоимость одного шага применения прореживания к конкретной модели пренебрежимо мала, его общая стоимость для модели в целом может оказаться значительной.} Будучи методом регуляризации, \emph{прореживание уменьшает эффективную емкость модели}. Чтобы компенсировать этот эффект, мы должны увеличить размер модели. Как правило, оптимальная ошибка на контрольном наборе при использовании прореживания намного ниже, но расплачиваться за это приходится гораздо большим размером модели и числом итераций алгоритма обучения.

NB: Для очень больших наборов данных регуляризация не сильно снижает ошибку обобщения. В таких случаях вычислительная стоимость прореживания и увеличения модели могут перевесить выигрыш от регуляризации.

\emph{Прореживание} регуляризует каждый скрытый блок, делая его не просто хорошим признаком, а \emph{хорошим в разных контекстах}.

Пакетная нормализация изменяет параметризацию модели, стремясь ввести в скрытые блоки как аддитивный, так и мультипликативный шум на этапе обучения. Основная цель \emph{пакетной нормализации} -- улучшить оптимизацию, но \emph{шум может давать регуляризующий эффект}, так что иногда прореживание оказывается лишним.










% Источники в "Газовой промышленности" нумеруются по мере упоминания 
\begin{thebibliography}{99}\addcontentsline{toc}{section}{Список литературы}
	\bibitem{ramalho:python-2022}{\emph{Рамальо Л.} Python -- к вершинам мастерства: Лаконичное и эффективное программирование. -- М.: МК Пресс, 2022. -- 898 с.}
	
	\bibitem{heydt:pandas-2019}{\emph{Хейдт М., Груздев А.} Изучаем pandas. -- М.: ДМК Пресс, 2019. -- 682 с.}
\end{thebibliography}

%\listoffigures\addcontentsline{toc}{section}{Список иллюстраций}

%\lstlistoflistings\addcontentsline{toc}{section}{Список листингов}

\end{document}
