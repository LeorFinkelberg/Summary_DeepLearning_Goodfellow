\documentclass[%
	11pt,
	a4paper,
	utf8,
	%twocolumn
]{article}	

\usepackage{style_packages/podvoyskiy_article_extended}


\begin{document}
\title{Конспект по книге Гудфеллоу <<Глубокое обучение>>\footnote{Гудфеллоу Я., Бенджио И., Курвилль А. Глубокое обучение. -- М.: ДМК Пресс, 2018. -- 652 с. }}

\author{}

\date{}
\maketitle

\thispagestyle{fancy}

\tableofcontents

\section{Численные методы}

\section{Основы машинного обучения}

\subsection{Точечная оценка}

Точечное оценивание -- это попытка найти единственное <<наилучшее>> предсказание интересующей величины. Пусть $ \{ x^{(1)}, \ldots, x^{(m)} \} $ -- множество $ m $ независимых и одинаково распределенных точек. \emph{Точечной оценкой}, или \emph{статистикой}, называется любая функция этих данных
\begin{align*}
	\theta_m = g(x^{(1)}, \ldots, x^{(m)}).
\end{align*}

В этом определении не требуется, чтобы $ g $ возвращала значение, близкое к истинному значению $ \theta $, ни даже чтобы область значений $ g $ совпадала со множеством допустимых значений~$ \theta $.

Алгоритм $ k $-групповой перекрестной проверки применяется для оценивания ошибки обобщения алгоритма обучения $ A $, когда имеющийся набор данных $ \mathbb{D} $ \emph{слишком мал} для того, чтобы простое разделение на обучающий и тестовый или обучающий и контрольный наборы могло дать точную оченку ошибки обобщения, поскольку среднее значение потери $ L $ на малом тестовом наборе может иметь высокую дисперсию. 

\subsection{Смещение}

\emph{Смещение оценки} определяется следующим образом
\begin{align*}
	\text{bias}(\hat{\theta}_m) = \mathbb{E}( \hat{\theta}_m ) - \theta, 
\end{align*}
где математической ожидание вычисляется по данным (рассматривается как выборка из случайной величины), а $ \theta $ -- истинное значение параметра, которое определяет порождающее распределение.

Оценка $ \hat{\theta} $ называется \emph{несмещенной}, если 
\begin{align*}
	\text(\hat{\theta}_m) = 0, \ \text{т.е.} \ \mathbb{E}(\hat{\theta}_m) = \theta.
\end{align*}

Оценка $ \hat{\theta}_m $ называется \emph{асимптотически несмещенной}, если
\begin{align*}
	\lim_{m \to \infty} \text{bias}(\hat{\theta}_m) = 0, \ \text{т.е.} \lim_{m \to \infty} \mathbb{E}(\hat{\theta}_m) = \theta.
\end{align*}


\subsection{Дисперсия}

Для определения смещения мы вычисляли математичесвкое ожидание оценки, но точно так же можем вычислить и ее дисперсию. \emph{Дисперсией оценки} называется выражение
\begin{align*}
	\text{Var}(\hat{\theta}).
\end{align*}

\emph{Стандартной ошибкой} $ \text{SE}(\hat{\theta}) $ называется квадратный корень из дисперсии.

Воспользовавшись центральной предельной теоремой, согласно которой среднее имеет приблизительно нормальное распределение, можем применить стандартную ошибку для вычисления вероятности того, что истинное математическое ожидание находится в выбранном интервале. Например, \emph{95-процентный доверительный интервал} вокруг выборочного среднего (вокру оценки) $ \hat{\mu}_m = \dfrac{1}{m} \sum\limits_{k=1}^{n} x^{(i)} $ определяется формулой
\begin{align*}
	(\hat{\mu}_m - 1.96 \, \text{SE} (\hat{\mu}_m), \hat{\mu}_m + 1.96 \, \text{SE} (\hat{\mu}_m))
\end{align*}
при нормальном распределении со средним $ \hat{\mu}_m $ и дисперсией $ \text{SE}(\hat{\mu}_m)^2 $.

NB: В экспериментах по машинному обучению принято говорить, что алгоритм $ A $ лучше алгоритма $ B $, если верхняя граница 95-процентного доверительного интервала для ошибки алгоритма $ A $ меньше нижней границы 95-процентного доверительного интервала для ошибки алгоритма $ B $.

\subsection{Поиск компромисса между смещением и дисперсией для минимизации среднеквадратической ошибки}

Что, если имеются две оценки, у одной из которых больше смещение, а у другой дисперсия? Какую выбрать? 

Самый распространенный подход к выбору компромиссного решения -- воспользоваться \emph{перекрестной проверкой}. Эмпирически продемонстрировано, что перекрестная проверка дает отличные результаты во многих реальных задачах.

Можно также сравнить среднекваратическую ошибку (MSE) обеих оценок
\begin{align*}
	\text{MSE} = \mathbb{E} [ ( \hat{\theta}_m - \theta )^2] = \text{bias}(\hat{\theta}_m)^2 + \text{Var}(\hat{\theta}_m)
\end{align*}

Желательной является оценка с малой MSE, именно такие оценки держат под контролем и смещение, и дисперсию. Соотношение между смещением и дисперсией тесно связано с возникающими в машинном обучении понятиями емкости модели, недообучения и переобучения.

Если ошибка обобщения измеряется посредством MSE (и тогда смещение и дисперсия становятся важными компонентами ошибки обобщения), то увеличение емкости (то есть \emph{усложение модели}) влечет за собой \emph{повышение дисперсии} и \emph{снижение смещения}.


\subsection{Состоятельность}

Обычно нас интересует также поведение оценки по мере роста размера обучающего набора. В частности, мы хотим, чтобы при увеличении числа примеров точечные оценки сходились к истинным значениям соответствующих параметров.

Формально это записывается в виде (условие состоятельности)
\begin{align*}
	\hat{\theta}_m \stackrel{\mathbf{P}}{\longrightarrow} \theta, \ (m \to \infty)
\end{align*}

Иногда это условие называют \emph{слабой состоятельностью}, понимая под \emph{сильной состоятельностью} сходимость \emph{почти наверное} $ \hat{\theta} $ к $ \theta $.

{\color{blue}Состоятельность гарантирует, что смещение оценки уменьшается с ростом числа примеров}. Однако обратное неверно -- {\color{red}из асимптотической несмещенности не вытекает состоятельность}. Рассмотрим, к примеру, оценивание среднего $ \mu $ нормального распределения $ N(x; \mu, \sigma^2) $ по набору данных, содержащему $ m $ примеров: $ \{ x^{(1)}, \ldots, x^{(m)} \} $.

Можно было бы взять в качестве оценки первый пример: $ \hat{\theta} = x^{(i)} $. В таком случае $ \mathbb{E}(\hat{\theta})_m = \theta $, поэтому оценка является несмещенной вне зависимости от того, сколько примеров мы видели. Отсюда, конечно, следует, что оценка асимптотически несмещенная. Но она не является состояительной, т.к. \emph{неверно}, что $ \hat{\theta}_m \to \theta, \ (m \to \infty) $.

\subsection{Оценка максимального праводподобия}

Рассмотрим множества $ m $ примеров $ \mathbb{X} = \{ x^{(1)}, \ldots, x^{(m)} \} $, независимо выбираемых из неизвестного порождающего распределения $ p_{data}(x) $.

Обозначим $ p_{model}(x; \theta) $ параметрическое семейство распределений вероятности над одним и тем же пространством, индексированное параметром $ \theta $. 

Тогда оценка максимального правдоподобия для $ \theta $ определяется формулой
\begin{align*}
	\theta_{ML} = \argmax_{\theta} \, p_{model} (\mathbb{X}; \theta) = \argmax_{\theta} \, \prod_{i=1}^{m} p_{model}(x^{(i)}; \theta)
\end{align*}

Такое произведение большого числа вероятностей по ряду причин может быть неудобно. Например, оно подвержено \emph{потере значимости}. Для получения эквивалентной, но более удобной задачи оптимизации заметим, что взятие логарифма правдоподобия не изменяет $ \argmax $, но преобразует произведение в сумму
\begin{align*}
	\theta_{ML} = \argmax_{\theta} \, \sum_{i=1}^{m} \log p_{model}(x^{(i)}; \theta)
\end{align*}

Поскольку $ \argmax $ не изменяется при умножении функции стоимости на константу, мы можем разделить правую часть на $ m $ и получить выражение в виде математического ожидания относительно эмпирического распределения $ \hat{p}_{data} $, определяемого обучающими данными
\begin{align*}
	\theta_{ML} = \argmax_{\theta} \, \mathbb{E}_{x \, \sim \, \hat{p}_{data}} [\, \log p_{model} (x; \theta) \,]
\end{align*}

{\color{blue}Один из способов интерпретации оценки максимального правдоподобия состоит в том, чтобы рассматривать ее как минимизацию дивергенции (расхождения) Кульбака-Лейблера между этими эмпирическим распределением $ \hat{p}_{data} $, определяемым обучающим набором, и модельным распределением.}

Дивергенция Кульбака-Лейблера определяется формулой
\begin{align*}
	D_{KL}(\hat{p}_{data} \, || \, p_{model}) = \mathbb{E}_{ x \, \sim \, \hat{p}_{data} } [\, \log \hat{p}_{data}(x) - \log p_{model}(x) \, ]
\end{align*}

Первый член разности в квадратных скобках зависит только от порождающего данные процесса, но не от модели. Следовательно, при обучении модели, минимизирующей дивергенцию КЛ, мы должны минимизировать только величину
\begin{align*}
	- \mathbb{E}_{ x \, \sim \, \hat{p}_{data} }[\, \log p_{model}(x) \,],
\end{align*}
а это, конечно, то же самое, что максимизация величины $ \theta_{ML} = \argmax_{\theta} \, \mathbb{E}_{x \, \sim \, \hat{p}_{data}} [\, \log p_{model} (x; \theta) \,] $.

NB: То есть, другими словами задача максимизации правдоподобия эквивалентна задаче минимизации дивергенции Кульбака-Лейблера между эмпирическим распределением $ \hat{p}_{data} $ и модельным распределением $ p_{model} $.

\subsection{Метод опорных векторов}

Линейную функцию в методе опорных векторов можно переписать в виде
\begin{align*}
	w^{T} x + b = b + \sum_{i=1}^{m} \alpha_i x^{T} x^{(i)},
\end{align*}
где $ x^{(i)} $ -- обучающий пример, $ \alpha $ -- вектор коэффициентов.

Записав алгоритм обучения в таком виде, мы сможем заменить $ x $ результатом заданной функции признаков $ \varphi(x) $, а скалярное произведение -- функцией $ k(x, x^{(i)}) = \varphi(x) \cdot \varphi(x^{(i)}) $, которая называется ядром.

Заменив скалярное произведение вычислением ядра, мы можем делать предсказание, пользуясь функцией
\begin{align*}
	f(x) = b + \sum_{i} \alpha_i k(x, x^{(i)})
\end{align*}

Основанная на ядре функция в точности эквивалентна предварительной обработке путем применения $ \varphi(x) $ ко всем входным данным с последующим обучением линейной модели в новом преобразованном пространстве.

NB: Трюк с ядром полезен по двум причинам:
\begin{itemize}
	\item Во-первых, он позволяет обучать модели, \emph{нелинейно} зависящие от $ x $, применяя методы выпуклой оптимизации, о которых точно известно, что они сходятся эффективно
	
	\item  Во-вторых, \emph{ядерная функция} $ k $ часто допускает реализацию, значительно \emph{более эффективную с вычислительной точки зрения}, чем наивное построение двух векторов $ \varphi(x) $ и явное вычисление их скалярного произведения
\end{itemize}	

Главный недостаток ядерных методов -- тот факт, что сложность вычисления решающей функции линейно зависит от числа обучающих примеров, поскольку $ i $-ый пример вносит член $ \alpha_i k(x, x^{(i)}) $ в решающую функцию.

В методе опорных векторов эта проблема сглаживается тем, что обучаемый вектор $ \alpha $ содержит в основном нули. {\color{blue}Тогда для классификации нового примера \emph{требуется вычислить ядерную функцию только для обучающих примеров с ненулевыми} $ \alpha_i $.} Эти обучающие примеры и называются опорными векторами.

\subsection{Метод главных компонент}

Метод главных компонент находит ортогональное линейное преобразование, переводящее входные данные $ x $ в представление $ z $. 

Рассмотрим матрицу плана $ X $ размера $ m \times n $. Будем предполагать, что матемаческое ожидание данных $ \mathbb{E}[x] = 0 $. Если это не так, центрирования легко добиться, вычтя среднее из всех примеров на этапе предварительной обработки.

\emph{Несмещенная выборочная ковариационная матрица}, ассоциированная с $ X $, определяется по формуле
\begin{align*}
	Var[x] = \dfrac{1}{m - 1} X^T X
\end{align*}

PCA находит представление (посредством линейного преобразования) $ z = W^T x $, для которого $ Var[z] $ -- \emph{диагональная}.

Главные компоненты можно получить с помощью сингулярного разложения. Точнее, это правые сингулярные веркторы. Чтобы убедиться в этом, предположим, что $ W $ -- правые сингулярные векторы в разложении $ X = U \Sigma W^T $. Тогда исходное уравнение собственных векторов можно переписать в базисе $ W $
\begin{align*}
	X^T X = (U \Sigma W^T)^T U \Sigma W^T = W \Sigma^2 W^T
\end{align*}

Разложение SVD полезно для доказательства того, что PCA приводит к диагональной матрице $ Var[z] $. Применяя сингулярное разложение $ X $, мы можем выразить дисперсию $ X $ в виде
\begin{align*}
	Var[x] = \dfrac{1}{m - 1} X^T X = \dfrac{1}{m - 1} (U \Sigma^2 W^T)^T U \Sigma W^T = \dfrac{1}{m - 1} W \Sigma^2 W^T,
\end{align*}
где используется тот факт, что $ U^T U = I $, поскольку матрца $ U $ в сингулярном разложении по определению ортогональная. Отсюда следует, что ковариационная матрица $ z $ диагональная
\begin{align*}
	Var[z] = \dfrac{1}{m - 1} Z^T Z = \dfrac{1}{m - 1}W^T X^T X W = \dfrac{1}{m-1} W^T W \Sigma^2 W^T W = \dfrac{1}{m - 1}\Sigma^2
\end{align*}

На этот раз мы воспользовались тем, что $ W^T W = I $ -- опять же по определению сингулярного разложения.

Проведенный анализ показывает, что {\color{blue}представление, полученное в результате проецирования данных $ x $ на $ z $ посредством линейного преобразования $ W $, имеет \emph{диагональную ковариационную матрицу} $ \Sigma^2 $. А отсюда сразу вытекает, что \emph{взаимная корреляция отдельных элементов} $ z $ \emph{равна нулю}.}

\subsection{Стохастический градиентный спуск}

Стохастический градиентный спуск (СГС) имеет важные применения и за пределами глубокого обучения. Это основной способ обучения \emph{больших линейных моделей} на очень больших наборах данных. Для модели фиксированноо размера стоимость одного шага СГС не зависит от размера обучающего набора $ m $. Количество шагов до достижения сходимости обычно возрастает с ростом размера обучающего набора. Но когда $ m $ стремится к бесконечности, модель в итоге сходится к наилучшей возможной ошибке тестирования еще до того, как СГС проверил каждый пример из обучающего набора. Дальнейшее увеличение $ m $ не увеличивает время обуччения, необходимое для достижения наилучшей ошибки тестирования. {\color{blue}С этой точки зрения можно сказать, что асимптотическая стоимость обучения модели методом СГС как функции от $ m $ имеет порядок~$ O(1) $.}

\subsection{Условное логарифмическое правдоподобие и среднеквадратическая ошибка}

\emph{Линейую регрессию} можно интерпретировать как \emph{нахождение оценки максимального правдоподобия}. Будем считать, что цель не в том, чтобы вернуть одно предсказание $ \hat{y} $, а чтобы построить модель, порождающую условное распределение $ p(y | \mathbf{x}) $. Цель алгоритма обучения теперь -- аппроксимировать $ p(y | \mathbf{x}) $, подогнав его под все эти разные значения $ y $, совместимые с $ \mathbf{x} $.

Для вывода такого же алгоритма линейной регрессии, как и раньше, определим
$$
p(y | \mathbf{x}) = N(y; \hat{y}(\mathbf{x}, \mathbf{w}), \sigma^2)
$$

Функкция $ \hat{y}(\mathbf{x}; \mathbf{w}) $ дает предсказание среднего значения нормального распределения. В этом примере мы предполагаем, что дисперсия фиксирована и равна константе $ \sigma^2 $. Поскольку предполагается, что примеры независимы и одинаково распределены, то условное логарифмическое правдоподобие записывается в виде
\begin{align*}
	\sum_{i=1}^{m} \log p(y^{(i)} | \mathbf{x}^{(i)}; {\theta}) = -m \, \log \sigma - \dfrac{m}{2} \log 2 \pi - \sum_{i=1}^{m} \dfrac{ \| \hat{y}^{(i)} - y^{(i)} \|^2 }{ 2 \sigma^2 },
\end{align*}
где $ \hat{y}^{(i)} $ -- результат линейной регрессии для $ i $-ого примера $ \mathbf{x}^{(i)} $, а $ m $ -- число обучающих примеров.

Сравнивая логарифмическое правдоподобие со среднеквадратической ошибкой
\begin{align*}
	\text{MSE}_{train} = \dfrac{1}{m} \sum_{i=1}^{m} \| \hat{y}^{(i)} - y^{(i)} \|^2,
\end{align*}
мы сразу же видим, что \emph{максимизация логарифмического правдоподобия} относительно $ \mathbf{w} $ дает ту же оценку параметров $ \mathbf{w} $, что \emph{минимизация среднеквадратической ошибки}.

Значения этих критериев различны, но положение оптимума совпадает. Это служит обоснованием использования среднеквадратической ошибки в качестве оценки максимального правдоподобия.

\section{Глубокие сети прямого распространения}

\subsection{Обучение условных распределений с помощью максимального правдоподобия}

Большинство современных нейронных сетей обучается \emph{с помощью максимального правдоподобия}. Это означает, что {\color{blue}в качестве функции стоимости берется отрицательное логарифмическое правдоподобие}, которое можно эквивалентно описать как перекрестную энтропию между обучающими данными и распределением модели
\begin{align*}
	J(\theta) = - \mathbb{E}_{x, y \sim \hat{p}_{\text{data}}} \log p_{\text{model}} (\mathbf{y} | \mathbf{x})
\end{align*}

Одно необычное свойство перекрестной энтропии, используемой при вычислении оценки максимального правдоподобия, заключается в том, что для типичных встречающихся на практике моделей у нее, как правило, нет минимального значения. Если выходная величина дискретна, то в большинстве моделей параметризация устроена так, что модель неспособна представить вероятность 0 или 1, но может подойти к ней сколь угодно близко. Примером может служить логистическая регрессия.

\subsection{Сигмоидные блоки и выходное распределение Бернулли}

Во многих задачах требуется предсказывать значение бинарной величины $ y $. В таком виде можно представить задачу классификации с двумя классами.

Подход на основе максимального правдоподобия заключается в определении распределения Бернулли величины $ y $ при условии $ \mathbf{x} $.

{\color{blue}Градиент $ \mathbf{0} $ обычно приводит к проблемам, потому что у алгоритма обучения нет никаких указаний на то, как улучшить параметры.}

Лучше применять другой подход, который гарантирует, что градиент обязательно будет достаточно большим, если модель дает неверный ответ. Этот подход основан на использовании сигмоидных выходных блоков в сочетании с максимальным правдоподобием.

Сигмоидный выходной блок
\begin{align*}
	\hat{y} = \sigma (\mathbf{w}^T \mathbf{h} + b),
\end{align*}
где $ \sigma $ -- логистическая сигмоида.










% Источники в "Газовой промышленности" нумеруются по мере упоминания 
\begin{thebibliography}{99}\addcontentsline{toc}{section}{Список литературы}
	\bibitem{ramalho:python-2022}{\emph{Рамальо Л.} Python -- к вершинам мастерства: Лаконичное и эффективное программирование. -- М.: МК Пресс, 2022. -- 898 с.}
	
	\bibitem{heydt:pandas-2019}{\emph{Хейдт М., Груздев А.} Изучаем pandas. -- М.: ДМК Пресс, 2019. -- 682 с.}
\end{thebibliography}

%\listoffigures\addcontentsline{toc}{section}{Список иллюстраций}

%\lstlistoflistings\addcontentsline{toc}{section}{Список листингов}

\end{document}
